{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Exercise: Starbucks\n",
    "<br>\n",
    "\n",
    "<img src=\"https://opj.ca/wp-content/uploads/2018/02/New-Starbucks-Logo-1200x969.jpg\" width=\"200\" height=\"200\">\n",
    "<br>\n",
    "<br>\n",
    " \n",
    "#### Background Information\n",
    "\n",
    "The dataset you will be provided in this portfolio exercise was originally used as a take-home assignment provided by Starbucks for their job candidates. The data for this exercise consists of about 120,000 data points split in a 2:1 ratio among training and test files. In the experiment simulated by the data, an advertising promotion was tested to see if it would bring more customers to purchase a specific product priced at $10. Since it costs the company 0.15 to send out each promotion, it would be best to limit that promotion only to those that are most receptive to the promotion. Each data point includes one column indicating whether or not an individual was sent a promotion for the product, and one column indicating whether or not that individual eventually purchased that product. Each individual also has seven additional features associated with them, which are provided abstractly as V1-V7.\n",
    "\n",
    "#### Optimization Strategy\n",
    "\n",
    "Your task is to use the training data to understand what patterns in V1-V7 to indicate that a promotion should be provided to a user. Specifically, your goal is to maximize the following metrics:\n",
    "\n",
    "* **Incremental Response Rate (IRR)** \n",
    "\n",
    "IRR depicts how many more customers purchased the product with the promotion, as compared to if they didn't receive the promotion. Mathematically, it's the ratio of the number of purchasers in the promotion group to the total number of customers in the purchasers group (_treatment_) minus the ratio of the number of purchasers in the non-promotional group to the total number of customers in the non-promotional group (_control_).\n",
    "\n",
    "$$ IRR = \\frac{purch_{treat}}{cust_{treat}} - \\frac{purch_{ctrl}}{cust_{ctrl}} $$\n",
    "\n",
    "\n",
    "* **Net Incremental Revenue (NIR)**\n",
    "\n",
    "NIR depicts how much is made (or lost) by sending out the promotion. Mathematically, this is 10 times the total number of purchasers that received the promotion minus 0.15 times the number of promotions sent out, minus 10 times the number of purchasers who were not given the promotion.\n",
    "\n",
    "$$ NIR = (10\\cdot purch_{treat} - 0.15 \\cdot cust_{treat}) - 10 \\cdot purch_{ctrl}$$\n",
    "\n",
    "For a full description of what Starbucks provides to candidates see the [instructions available here](https://drive.google.com/open?id=18klca9Sef1Rs6q8DW4l7o349r8B70qXM).\n",
    "\n",
    "Below you can find the training data provided.  Explore the data and different optimization strategies.\n",
    "\n",
    "#### How To Test Your Strategy?\n",
    "\n",
    "When you feel like you have an optimization strategy, complete the `promotion_strategy` function to pass to the `test_results` function.  \n",
    "From past data, we know there are four possible outomes:\n",
    "\n",
    "Table of actual promotion vs. predicted promotion customers:  \n",
    "\n",
    "<table>\n",
    "<tr><th></th><th colspan = '2'>Actual</th></tr>\n",
    "<tr><th>Predicted</th><th>Yes</th><th>No</th></tr>\n",
    "<tr><th>Yes</th><td>I</td><td>II</td></tr>\n",
    "<tr><th>No</th><td>III</td><td>IV</td></tr>\n",
    "</table>\n",
    "\n",
    "The metrics are only being compared for the individuals we predict should obtain the promotion â€“ that is, quadrants I and II.  Since the first set of individuals that receive the promotion (in the training set) receive it randomly, we can expect that quadrants I and II will have approximately equivalent participants.  \n",
    "\n",
    "Comparing quadrant I to II then gives an idea of how well your promotion strategy will work in the future. \n",
    "\n",
    "Get started by reading in the data below.  See how each variable or combination of variables along with a promotion influences the chance of purchasing.  When you feel like you have a strategy for who should receive a promotion, test your strategy against the test dataset used in the final `test_results` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Promotion</th>\n",
       "      <th>purchase</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>30.443518</td>\n",
       "      <td>-1.165083</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>32.159350</td>\n",
       "      <td>-0.645617</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>30.431659</td>\n",
       "      <td>0.133583</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.588914</td>\n",
       "      <td>-0.212728</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>28.044331</td>\n",
       "      <td>-0.385883</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>41.189415</td>\n",
       "      <td>-0.905350</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40.690409</td>\n",
       "      <td>1.085939</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>25.971529</td>\n",
       "      <td>-1.424817</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>31.930423</td>\n",
       "      <td>0.393317</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27.474650</td>\n",
       "      <td>0.566472</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID Promotion  purchase  V1         V2        V3  V4  V5  V6  V7\n",
       "0   1        No         0   2  30.443518 -1.165083   1   1   3   2\n",
       "1   3        No         0   3  32.159350 -0.645617   2   3   2   2\n",
       "2   4        No         0   2  30.431659  0.133583   1   1   4   2\n",
       "3   5        No         0   0  26.588914 -0.212728   2   1   4   2\n",
       "4   8       Yes         0   3  28.044331 -0.385883   1   1   2   2\n",
       "5   9        No         0   1  41.189415 -0.905350   2   4   4   1\n",
       "6  11        No         0   1  40.690409  1.085939   2   2   1   2\n",
       "7  14        No         0   2  25.971529 -1.424817   1   3   3   2\n",
       "8  15       Yes         0   2  31.930423  0.393317   2   3   1   2\n",
       "9  16        No         0   1  27.474650  0.566472   2   2   3   2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in packages\n",
    "from itertools import combinations\n",
    "\n",
    "from test_results import test_results, score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# hyperparameter search library\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# scoring libraries\n",
    "from sklearn.metrics import make_scorer, f1_score, classification_report\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# load in the data\n",
    "train_data = pd.read_csv('./training.csv')\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create copy of training data and clean ID\n",
    "train_data2=train_data.copy()\n",
    "train_data2.drop(columns=[\"ID\"],inplace=True)  # Kick out ID\n",
    "\n",
    "# Replace \"Yes\" and \"No\" in \"Promotions\" column with 1 and 0, respectively\n",
    "f=lambda x : (1 if x==\"Yes\" else 0)\n",
    "train_data2.Promotion=np.array([f(train_data2.Promotion.iloc[i]) for i in range(0,len(train_data2))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subsets for all combinations of \"Promotion\", \"purchase\"\n",
    "\n",
    "subsetno=180000\n",
    "train_data_sure_buyer=sk.utils.shuffle(train_data2[(train_data2.purchase==1) & (train_data2.Promotion==0)]).head(subsetno)\n",
    "train_data_sure_not_buyer=sk.utils.shuffle(train_data2[(train_data2.purchase==0) & (train_data2.Promotion==1)]).head(subsetno)\n",
    "train_data_promoted_buyer=sk.utils.shuffle(train_data2[(train_data2.purchase==1) & (train_data2.Promotion==1)]).head(subsetno)\n",
    "train_data_unpromoted_not_buyer=sk.utils.shuffle(train_data2[(train_data2.purchase==0) & (train_data2.Promotion==0)]).head(subsetno)\n",
    "\n",
    "\n",
    "# adjust dataframes\n",
    "train_data_sure_buyer.drop(columns=['Promotion','purchase'],inplace=True)\n",
    "train_data_sure_buyer['parameter']=0\n",
    "\n",
    "train_data_sure_not_buyer.drop(columns=['Promotion','purchase'],inplace=True)\n",
    "train_data_sure_not_buyer['parameter']=0\n",
    "\n",
    "train_data_promoted_buyer.drop(columns=['Promotion','purchase'],inplace=True)\n",
    "train_data_promoted_buyer['parameter']=1\n",
    "\n",
    "train_data_unpromoted_not_buyer.drop(columns=['Promotion','purchase'],inplace=True)\n",
    "train_data_unpromoted_not_buyer['parameter']=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(319, 8)\n",
      "(41643, 8)\n",
      "(721, 8)\n",
      "(41851, 8)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_sure_buyer.shape)\n",
    "print(train_data_sure_not_buyer.shape)\n",
    "\n",
    "print(train_data_promoted_buyer.shape)\n",
    "\n",
    "print(train_data_unpromoted_not_buyer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2 more subsets in which those who do not need a promotion have a value of zero and those who need or perhaps (we do not know) may not need a promotion get value 1.\n",
    "\n",
    "train_data_sure_buyer_and_unpromoted_not_buyer=pd.concat([train_data_sure_buyer, train_data_unpromoted_not_buyer])\n",
    "train_data_sure_not_buyer_and_promoted_buyer=pd.concat([train_data_sure_not_buyer, train_data_promoted_buyer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build train and test set for train_data_sure_buyer_and_unpromoted_not_buyer\n",
    "X_train=train_data_sure_buyer_and_unpromoted_not_buyer.iloc[:,:7]\n",
    "y_train=np.array(train_data_sure_buyer_and_unpromoted_not_buyer.iloc[:,7:8].values).ravel()\n",
    "\n",
    "# build train and test set for train_data_sure_not_buyer_and_promoted_buyer\n",
    "X_train2=train_data_sure_not_buyer_and_promoted_buyer.iloc[:,:7]\n",
    "y_train2=np.array(train_data_sure_not_buyer_and_promoted_buyer.iloc[:,7:8].values).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# library for estimator base class\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "def combine_probabilities_into_output(prob_a,prob_b):\n",
    "    \"\"\"\n",
    "    It is aimed to combine two preditor probabilities into one\n",
    "    \n",
    "    INPUT\n",
    "    prob_a - probability of event a\n",
    "    prob_b - probability of event b\n",
    "    \n",
    "    OUTPUT\n",
    "    \"YES\" or \"NO\" depending on the combined condition\n",
    "    \"\"\"\n",
    "    # if ((prob_a>0.5) & (prob_b>0.5)):\n",
    "    if ((prob_a+prob_b>1.0)&(prob_a>0.43) & (prob_b>0.565)):\n",
    "        return \"Yes\"\n",
    "    else:\n",
    "        return \"No\"\n",
    "\n",
    "class own_classifier(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, tempi=0, estimator=[LogisticRegression(),LogisticRegression()], C_param=100):\n",
    "        \"\"\"constructor function\"\"\"\n",
    "        print(\"konstruktor: \", estimator)\n",
    "        \n",
    "        self.set_params(tempi,estimator, C_param)\n",
    "        return None\n",
    "\n",
    "    def set_params(self, tempi=0,estimator=[LogisticRegression(),LogisticRegression()], C_param=100):\n",
    "        \"\"\"parameters are set\"\"\"\n",
    "        \n",
    "        print(\"zuerst: \",estimator)\n",
    "        self.estimator = estimator# [estimator[0],estimator[1]]\n",
    "        self.C_param = C_param\n",
    "        self.classes_ = [0, 1]\n",
    "        self.tempi=tempi\n",
    "        print(self.estimator)\n",
    "        self.estimator[0].set_params( random_state=0, C=self.C_param, max_iter=5000, class_weight=\"balanced\" )\n",
    "        self.estimator[1].set_params( random_state=0, C=self.C_param, max_iter=5000, class_weight=\"balanced\" )\n",
    "        # self.model = [ self.estimator[0],self.estimator[1]]\n",
    "        print(\"in set_params proc\", self.tempi, \"fghjk\")\n",
    "        return self\n",
    "    \n",
    "    def combine_probabilities_into_output(prob_a,prob_b):\n",
    "        \"\"\"\n",
    "        It is aimed to combine two preditor probabilities into one\n",
    "        \n",
    "        INPUT\n",
    "        prob_a - probability of event a\n",
    "        prob_b - probability of event b\n",
    "        \n",
    "        OUTPUT\n",
    "        \"YES\" or \"NO\" depending on the combined condition\n",
    "        \"\"\"\n",
    "        # if ((prob_a>0.5) & (prob_b>0.5)):\n",
    "        if ((prob_a+prob_b>1.0)&(prob_a>0.43) & (prob_b>0.565)):\n",
    "            return \"Yes\"\n",
    "        else:\n",
    "            return \"No\"\n",
    "\n",
    "    def score(self, X, y):\n",
    "       \"\"\"scoring function\"\"\"\n",
    "       return self.model[0].score(X[0], y[0])+self.model[1].score(X[1], y[1])\n",
    "    #def __sklearn_clone__(self):    \n",
    "    #    return self\n",
    "    def fit(self,X2,y=None):\n",
    "        \"\"\"fit procedure covering all classifiers for each target\"\"\"\n",
    "\n",
    "        X=X2.copy()\n",
    "        train_data_sure_buyer=sk.utils.shuffle(X[(X.purchase==1) & (X.Promotion==0)])\n",
    "        train_data_sure_not_buyer=sk.utils.shuffle(X[(X.purchase==0) & (X.Promotion==1)])\n",
    "        train_data_promoted_buyer=sk.utils.shuffle(X[(X.purchase==1) & (X.Promotion==1)])\n",
    "        train_data_unpromoted_not_buyer=sk.utils.shuffle(X[(X.purchase==0) & (X.Promotion==0)])\n",
    "\n",
    "\n",
    "        # adjust dataframes\n",
    "        train_data_sure_buyer.drop(columns=['Promotion','purchase'],inplace=True)\n",
    "        train_data_sure_buyer['parameter']=0\n",
    "\n",
    "        train_data_sure_not_buyer.drop(columns=['Promotion','purchase'],inplace=True)\n",
    "        train_data_sure_not_buyer['parameter']=0\n",
    "\n",
    "        train_data_promoted_buyer.drop(columns=['Promotion','purchase'],inplace=True)\n",
    "        train_data_promoted_buyer['parameter']=1\n",
    "\n",
    "        train_data_unpromoted_not_buyer.drop(columns=['Promotion','purchase'],inplace=True)\n",
    "        train_data_unpromoted_not_buyer['parameter']=1\n",
    "\n",
    "\n",
    " \n",
    "        print(\"in fit proc\")\n",
    "        \n",
    "        # print(len(y))\n",
    "        # print(y)\n",
    "        \n",
    "        \n",
    "        # self.model = [ self.estimator[0]( random_state=0, C=self.C_param, max_iter=5000, class_weight=\"balanced\" ),  self.estimator[1](  random_state=0, C=self.C_param, max_iter=5000, class_weight=\"balanced\"  )     ]\n",
    "        # self.model[0].fit(X[0], y[0])\n",
    "        # self.model[1].fit(X[1], y[1])\n",
    "\n",
    "        # create 2 more subsets in which those who do not need a promotion have a value of zero and those who need or perhaps (we do not know) may not need a promotion get value 1.\n",
    "\n",
    "        train_data_sure_buyer_and_unpromoted_not_buyer=pd.concat([train_data_sure_buyer, train_data_unpromoted_not_buyer])\n",
    "        train_data_sure_not_buyer_and_promoted_buyer=pd.concat([train_data_sure_not_buyer, train_data_promoted_buyer])\n",
    "\n",
    "        # build train and test set for train_data_sure_buyer_and_unpromoted_not_buyer\n",
    "        X_train=train_data_sure_buyer_and_unpromoted_not_buyer[['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7']]\n",
    "        y_train=np.array(train_data_sure_buyer_and_unpromoted_not_buyer[['parameter']].values).ravel()\n",
    "\n",
    "        # build train and test set for train_data_sure_not_buyer_and_promoted_buyer\n",
    "        X_train2=train_data_sure_not_buyer_and_promoted_buyer[['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7']]\n",
    "        y_train2=np.array(train_data_sure_not_buyer_and_promoted_buyer[['parameter']].values).ravel()\n",
    "        self.estimator[0].fit(X_train,y_train)\n",
    "        self.estimator[1].fit(X_train2,y_train2)\n",
    "\n",
    "        return self\n",
    "\n",
    "    # def fit(self, X, y=None):\n",
    "    #     \"\"\"fit procedure covering all classifiers for each target\"\"\"\n",
    "    #     f1 = y.sum() / y.shape[0]\n",
    "    #     if f1 == 0:\n",
    "    #         self.one_class_only = True\n",
    "    #         self.one_class_value = 0\n",
    "    #         return self\n",
    "    #     elif f1 == 1:\n",
    "    #         self.one_class_only = True\n",
    "    #         self.one_class_value = 1\n",
    "    #         return self\n",
    "    #     else:\n",
    "    #         self.one_class_only = False\n",
    "\n",
    "    #     self.model = OneVsRestClassifier(\n",
    "    #         self.estimator(\n",
    "    #             random_state=0, C=self.C_param, max_iter=5000, class_weight=\"balanced\"\n",
    "    #         ),\n",
    "    #         n_jobs=-1\n",
    "    #     )\n",
    "    #     self.model.fit(X, y)\n",
    "    #     return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"predict-funtion for the only one target value\"\"\"\n",
    "        # if self.one_class_only:\n",
    "        #     return [self.one_class_value for i in range(X.shape[0])]\n",
    "        #print(X.columns)\n",
    "        # X=X2.copy()\n",
    "        # X.drop(columns=['Promotion','purchase'],inplace=True)\n",
    "        y_pred = self.estimator[0].predict_proba(X.iloc[:,:]).T[1]\n",
    "        y_pred2 = self.estimator[1].predict_proba(X.iloc[:,:]).T[1]\n",
    "        print(y_pred)\n",
    "        print(y_pred2)\n",
    "        return np.array(list(map(combine_probabilities_into_output,y_pred,y_pred2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "konstruktor:  [LogisticRegression(), LogisticRegression()]\n",
      "zuerst:  [LogisticRegression(), LogisticRegression()]\n",
      "[LogisticRegression(), LogisticRegression()]\n",
      "in set_params proc 0 fghjk\n",
      "in fit proc\n",
      "konstruktor:  [LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "zuerst:  [LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "[LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "in set_params proc 0 fghjk\n",
      "[0.55499312 0.52833234 0.52657368 0.4775765  0.5536367 ]\n",
      "[0.27674068 0.56908162 0.24389393 0.54537198 0.25055352]\n",
      "hjk\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Promotion</th>\n",
       "      <th>purchase</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>30.443518</td>\n",
       "      <td>-1.165083</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>32.159350</td>\n",
       "      <td>-0.645617</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>30.431659</td>\n",
       "      <td>0.133583</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.588914</td>\n",
       "      <td>-0.212728</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>28.044331</td>\n",
       "      <td>-0.385883</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84529</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.084876</td>\n",
       "      <td>1.345672</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84530</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>33.501485</td>\n",
       "      <td>-0.299306</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84531</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>31.492019</td>\n",
       "      <td>1.085939</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84532</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37.766106</td>\n",
       "      <td>0.999361</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84533</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21.720835</td>\n",
       "      <td>1.085939</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84534 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Promotion  purchase  V1         V2        V3  V4  V5  V6  V7\n",
       "0              0         0   2  30.443518 -1.165083   1   1   3   2\n",
       "1              0         0   3  32.159350 -0.645617   2   3   2   2\n",
       "2              0         0   2  30.431659  0.133583   1   1   4   2\n",
       "3              0         0   0  26.588914 -0.212728   2   1   4   2\n",
       "4              1         0   3  28.044331 -0.385883   1   1   2   2\n",
       "...          ...       ...  ..        ...       ...  ..  ..  ..  ..\n",
       "84529          0         0   1  30.084876  1.345672   1   1   3   1\n",
       "84530          1         0   3  33.501485 -0.299306   1   1   4   1\n",
       "84531          0         0   1  31.492019  1.085939   2   3   2   2\n",
       "84532          0         0   1  37.766106  0.999361   2   2   1   2\n",
       "84533          0         0   1  21.720835  1.085939   2   2   1   2\n",
       "\n",
       "[84534 rows x 9 columns]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tr=own_classifier()\n",
    "own_classifier().fit(train_data2)\n",
    "own_classifier().predict(train_data2.iloc[0:5,2:9])\n",
    "# own_classifier().fit(X_train, y_train)\n",
    "print(\"hjk\")\n",
    "train_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"Definition of the model via pipeline and a parameters variable.\n",
    "    The best parameters were determined using grid search.\n",
    "    In this example, the bag of words approach and the word-to-vetor approach are\n",
    "    \"\"\"\n",
    "\n",
    "    pipeline= Pipeline([\n",
    "        ('clf',own_classifier())\n",
    "    ])\n",
    "\n",
    "\n",
    "    parameters = [\n",
    "        {\n",
    "            #\"clf__estimator\": ([LogisticRegression,LogisticRegression])\n",
    "            \"clf__tempi\": ([1])\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    cv = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid=parameters,\n",
    "        refit=True,\n",
    "        # scoring=make_scorer(f1_score, **dict(average=\"macro\", pos_label=1, zero_division=0))\n",
    "    )\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "konstruktor:  [LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "zuerst:  [LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "[LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "in set_params proc 0 fghjk\n",
      "hshs  Pipeline(steps=[('clf', own_classifier())])\n",
      "  \n",
      "konstruktor:  [LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "zuerst:  [LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "[LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "in set_params proc 0 fghjk\n",
      "konstruktor:  [LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "zuerst:  [LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "[LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "in set_params proc 0 fghjk\n",
      "zuerst:  [LogisticRegression(), LogisticRegression()]\n",
      "[LogisticRegression(), LogisticRegression()]\n",
      "in set_params proc 1 fghjk\n",
      "in fit proc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"g:\\python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 765, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\python\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 444, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\python\\Lib\\site-packages\\sklearn\\pipeline.py\", line 722, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\omf2\\AppData\\Local\\Temp\\ipykernel_19476\\4167347458.py\", line 68, in score\n",
      "    return self.model[0].score(X[0], y[0])+self.model[1].score(X[1], y[1])\n",
      "           ^^^^^^^^^^\n",
      "AttributeError: 'own_classifier' object has no attribute 'model'\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "konstruktor:  [LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "zuerst:  [LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "[LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "in set_params proc 0 fghjk\n",
      "zuerst:  [LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "[LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "in set_params proc 1 fghjk\n",
      "in fit proc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"g:\\python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 765, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\python\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 444, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\python\\Lib\\site-packages\\sklearn\\pipeline.py\", line 722, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\omf2\\AppData\\Local\\Temp\\ipykernel_19476\\4167347458.py\", line 68, in score\n",
      "    return self.model[0].score(X[0], y[0])+self.model[1].score(X[1], y[1])\n",
      "           ^^^^^^^^^^\n",
      "AttributeError: 'own_classifier' object has no attribute 'model'\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "konstruktor:  [LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "zuerst:  [LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "[LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "in set_params proc 0 fghjk\n",
      "zuerst:  [LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "[LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "in set_params proc 1 fghjk\n",
      "in fit proc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"g:\\python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 765, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\python\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 444, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\python\\Lib\\site-packages\\sklearn\\pipeline.py\", line 722, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\omf2\\AppData\\Local\\Temp\\ipykernel_19476\\4167347458.py\", line 68, in score\n",
      "    return self.model[0].score(X[0], y[0])+self.model[1].score(X[1], y[1])\n",
      "           ^^^^^^^^^^\n",
      "AttributeError: 'own_classifier' object has no attribute 'model'\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "konstruktor:  [LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "zuerst:  [LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "[LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "in set_params proc 0 fghjk\n",
      "zuerst:  [LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "[LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "in set_params proc 1 fghjk\n",
      "in fit proc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"g:\\python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 765, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\python\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 444, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\python\\Lib\\site-packages\\sklearn\\pipeline.py\", line 722, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\omf2\\AppData\\Local\\Temp\\ipykernel_19476\\4167347458.py\", line 68, in score\n",
      "    return self.model[0].score(X[0], y[0])+self.model[1].score(X[1], y[1])\n",
      "           ^^^^^^^^^^\n",
      "AttributeError: 'own_classifier' object has no attribute 'model'\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "konstruktor:  [LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "zuerst:  [LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "[LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "in set_params proc 0 fghjk\n",
      "zuerst:  [LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "[LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "in set_params proc 1 fghjk\n",
      "in fit proc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"g:\\python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 765, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\python\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 444, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\python\\Lib\\site-packages\\sklearn\\pipeline.py\", line 722, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\omf2\\AppData\\Local\\Temp\\ipykernel_19476\\4167347458.py\", line 68, in score\n",
      "    return self.model[0].score(X[0], y[0])+self.model[1].score(X[1], y[1])\n",
      "           ^^^^^^^^^^\n",
      "AttributeError: 'own_classifier' object has no attribute 'model'\n",
      "\n",
      "  warnings.warn(\n",
      "g:\\python\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "konstruktor:  [LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "zuerst:  [LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "[LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "in set_params proc 0 fghjk\n",
      "zuerst:  [LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "[LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "in set_params proc 1 fghjk\n",
      "konstruktor:  [LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "zuerst:  [LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "[LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0), LogisticRegression(C=100, class_weight='balanced', max_iter=5000,\n",
      "                   random_state=0)]\n",
      "in set_params proc 1 fghjk\n",
      "in fit proc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=Pipeline(steps=[(&#x27;clf&#x27;, own_classifier())]),\n",
       "             param_grid=[{&#x27;clf__tempi&#x27;: [1]}])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=Pipeline(steps=[(&#x27;clf&#x27;, own_classifier())]),\n",
       "             param_grid=[{&#x27;clf__tempi&#x27;: [1]}])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;clf&#x27;, own_classifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">own_classifier</label><div class=\"sk-toggleable__content\"><pre>own_classifier()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('clf', own_classifier())]),\n",
       "             param_grid=[{'clf__tempi': [1]}])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define and fit pipelines\n",
    "\n",
    "\n",
    "\n",
    "# # build pipeline\n",
    "# pipeline= Pipeline([\n",
    "#     ('clf',LogisticRegression())\n",
    "# ])\n",
    "# pipeline2= Pipeline([\n",
    "#     ('clf',LogisticRegression())\n",
    "# ])\n",
    "\n",
    "# ######\n",
    "\n",
    "\n",
    "\n",
    "#fit pipelines\n",
    "cv=build_model()\n",
    "print(\"hshs \",cv.estimator)\n",
    "print(\"  \")\n",
    "cv.fit(train_data2)\n",
    "# pipeline2.fit(X_train2,y_train2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promotion_strategy(df):\n",
    "    '''\n",
    "    INPUT\n",
    "    df - a dataframe with *only* the columns V1 - V7 (same as train_data)\n",
    "\n",
    "    OUTPUT\n",
    "    promotion_df - np.array with the values\n",
    "                   'Yes' or 'No' related to whether or not an\n",
    "                   individual should recieve a promotion\n",
    "                   should be the length of df.shape[0]\n",
    "\n",
    "    Ex:\n",
    "    INPUT: df\n",
    "\n",
    "    V1\tV2\t  V3\tV4\tV5\tV6\tV7\n",
    "    2\t30\t-1.1\t1\t1\t3\t2\n",
    "    3\t32\t-0.6\t2\t3\t2\t2\n",
    "    2\t30\t0.13\t1\t1\t4\t2\n",
    "\n",
    "    OUTPUT: promotion\n",
    "\n",
    "    array(['Yes', 'Yes', 'No'])\n",
    "    indicating the first two users would recieve the promotion and\n",
    "    the last should not.\n",
    "    '''\n",
    "    print(df.columns)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return cv.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7'], dtype='object')\n",
      "[0.47028547 0.46250006 0.51778756 ... 0.48797155 0.53971578 0.44025256]\n",
      "[0.21856772 0.53414189 0.62819958 ... 0.49044455 0.27178923 0.25453106]\n",
      "['No' 'No' 'Yes' ... 'No' 'No' 'No']\n",
      "2        Yes\n",
      "3         No\n",
      "5        Yes\n",
      "9         No\n",
      "10        No\n",
      "        ... \n",
      "41637     No\n",
      "41638    Yes\n",
      "41640    Yes\n",
      "41643     No\n",
      "41646    Yes\n",
      "Name: Promotion, Length: 12704, dtype: object\n",
      "ghjK 6388\n",
      "ghjK 46\n",
      "0.01892312731717125\n",
      "Nice job!  See how well your strategy worked on our test data below!\n",
      "\n",
      "Your irr with this strategy is 0.0189.\n",
      "\n",
      "Your nir with this strategy is 242.60.\n",
      "We came up with a model with an irr of 0.0188 and an nir of 189.45 on the test set.\n",
      "\n",
      " How did you do?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.01892312731717125, 242.60000000000002)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will test your results, and provide you back some information \n",
    "# on how well your promotion_strategy will work in practice\n",
    "\n",
    "test_results(promotion_strategy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
